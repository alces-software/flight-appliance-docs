.. _using-mpich:

Using MPICH with an Alces Flight Compute environment
====================================================

The following guide will detail the basic usage of the MPICH MPI library for Linux, covering both the installation of the MPICH Gridware package as well as basic usage. 

Prerequisites
-------------

*  Access to an Alces Flight Compute environment
*  Appropriate administrator access to install Gridware packages

Installing MPICH to your environment
------------------------------------

The MPICH Gridware package is easily installed using the ``alces gridware`` utility. To install ``mpich`` to your environment, run the following command as a privileged user: 

.. code:: bash

    [alces@login1(cluster1) ~]$ alces gridware install mpich
    Installing base/mpi/mpich/3.0.4
    
     > Preparing package sources
            Download --> mpich-3.0.4.tar.gz ... OK
              Verify --> mpich-3.0.4.tar.gz ... OK
    
     > Preparing for installation
               Mkdir ... OK (/var/cache/gridware/src/mpi/mpich/3.0.4/gcc-4.8.5)
             Extract ... OK
    
     > Proceeding with installation
             Compile ... OK
               Mkdir ... OK (/opt/gridware/depots/17145e23/el7/pkg/mpi/mpich/3.0.4/gcc-4.8.5)
             Install ... OK
              Module ... OK

Running an example job script with MPICH
----------------------------------------

To run an example job script using MPICH and multiple compute hosts, create a sample job script: 

.. code:: bash

    #!/bin/bash -l
    #$ -pe mpinodes-verbose 10
    #$ -N mpi-test -o $HOME/mpi.$JOB_ID.out
    module load mpi/mpich
    mpiexec -np 10 -machinefile /tmp/sge.machines.$JOB_ID echo "Hello from $HOSTNAME"

The ``mpich`` package should be called using ``mpiexec``, followed by any optional arguments you may wish to provide. In the above job script, we have provided: 

*  ``-np``: The number of processes to start
*  ``-machinefile``: The list of hosts to use. This is automatically generated by the scheduler and placed in the ``/tmp`` dir, in the format ``sge.machines.$JOB_ID``

Once your job script is complete - you can submit it to the scheduler. Once the job has completed, the output file will look like the following, displaying each of the hosts the job ran on: 

.. code:: bash

    =======================================================
    SGE job submitted on Tue 22 Mar 15:24:18 GMT 2016
    10 hosts used
    JOB ID: 5
    JOB NAME: mpi-test
    PE: mpinodes-verbose
    QUEUE: bynode.q
    MASTER node05.novalocal
    Nodes used:
    node05 node01 node06 node09 node00
    node04 node02 node08 node03 node07
    =======================================================
    
    ** A machine file has been written to /tmp/sge.machines.5 on node05.novalocal **
    
    =======================================================
    If an output file was specified on job submission
    Job Output Follows:
    =======================================================
    
    Hello from node05
    Hello from node04
    Hello from node02
    Hello from node08
    Hello from node01
    Hello from node09
    Hello from node00
    Hello from node03
    Hello from node06
    Hello from node07
    ==================================================
    SGE job completed on Tue 22 Mar 15:24:18 GMT 2016
    ==================================================


Your applications can also be run using MPICH, by loading both your application module together with the MPICH module - and running in parallel with the ``mpiexec <options> appname`` command. 
